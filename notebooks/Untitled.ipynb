{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T09:02:22.480566Z",
     "start_time": "2021-11-15T09:02:18.668584Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW\n",
    "\n",
    "sys.path.append(\"D:/_jupyter/kaggle/\")\n",
    "from src.data.jigsaw_dataset import JigsawDataset\n",
    "from src.utils.common import set_seed, get_hash_name\n",
    "\n",
    "HASH_NAME = get_hash_name(size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T09:02:30.709493Z",
     "start_time": "2021-11-15T09:02:22.482570Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbigshanedogg\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/bigshanedogg/jigsaw/runs/3qhoj8tt\" target=\"_blank\">lyric-darkness-25</a></strong> to <a href=\"https://wandb.ai/bigshanedogg/jigsaw\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "PROJECT_NAME = \"jigsaw\"\n",
    "wandb.init(project=PROJECT_NAME)\n",
    "anony = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T09:02:38.691921Z",
     "start_time": "2021-11-15T09:02:30.711493Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 20211115,\n",
    "    \"data_path\": \"../data/jigsaw-toxic-severity-rating/validation_data.csv\",\n",
    "    \"encoding\": \"utf-8\", \n",
    "    \"extension\": \"csv\",\n",
    "    \"epochs\": 3,\n",
    "    \"model_name\": \"roberta-base\",\n",
    "    \"train_batch_size\": 32,\n",
    "    \"valid_batch_size\": 32,\n",
    "    \"timesteps\": 128,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"scheduler\": 'CosineAnnealingLR',\n",
    "    \"min_lr\": 1e-6,\n",
    "    \"T_max\": 500,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"n_fold\": 5,\n",
    "    \"n_accumulate\": 1,\n",
    "    \"num_classes\": 1,\n",
    "    \"margin\": 0.5,\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"nprocs\": 1, \n",
    "    \"hash_name\": HASH_NAME\n",
    "}\n",
    "\n",
    "CONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "CONFIG['group'] = f'{HASH_NAME}-Baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T09:03:01.824588Z",
     "start_time": "2021-11-15T09:02:38.696211Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing data:   0%|                                                                    | 0/30108 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (743 > 512). Running this sequence through the model will result in indexing errors\n",
      "Preprocessing data: 100%|██████████████████████████████████████████████████████| 30108/30108 [00:22<00:00, 1313.52it/s]\n"
     ]
    }
   ],
   "source": [
    "set_seed(CONFIG['seed'])\n",
    "jigsaw_dataset = JigsawDataset(data_path=CONFIG[\"data_path\"], tokenizer=CONFIG[\"tokenizer\"], timesteps=CONFIG[\"timesteps\"], batch_size=CONFIG[\"train_batch_size\"], device=CONFIG[\"device\"], nprocs=CONFIG[\"nprocs\"], encoding=CONFIG[\"encoding\"], extension=CONFIG[\"extension\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(JigsawModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(768, CONFIG['num_classes'])\n",
    "        \n",
    "    def forward(self, ids, mask):        \n",
    "        out = self.model(input_ids=ids,attention_mask=mask,\n",
    "                         output_hidden_states=False)\n",
    "        out = self.drop(out[1])\n",
    "        outputs = self.fc(out)\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
