wandb_version: 1

T_max:
  desc: null
  value: 500
_wandb:
  desc: null
  value:
    cli_version: 0.12.6
    framework: huggingface
    huggingface_version: 4.9.2
    is_jupyter_run: true
    is_kaggle_kernel: false
    python_version: 3.8.2
    start_time: 1636946333
    t:
      1:
      - 1
      - 5
      - 11
      2:
      - 1
      - 5
      - 11
      3:
      - 1
      - 13
      - 15
      - 16
      4: 3.8.2
      5: 0.12.6
      6: 4.9.2
      8:
      - 1
      - 3
      - 5
device:
  desc: null
  value: cuda:0
epochs:
  desc: null
  value: 3
group:
  desc: null
  value: 85p02jdqiqnz-Baseline
hash_name:
  desc: null
  value: 85p02jdqiqnz
learning_rate:
  desc: null
  value: 0.0001
margin:
  desc: null
  value: 0.5
max_length:
  desc: null
  value: 128
min_lr:
  desc: null
  value: 1.0e-06
model_name:
  desc: null
  value: roberta-base
n_accumulate:
  desc: null
  value: 1
n_fold:
  desc: null
  value: 5
num_classes:
  desc: null
  value: 1
scheduler:
  desc: null
  value: CosineAnnealingLR
seed:
  desc: null
  value: 2021
tokenizer:
  desc: null
  value: 'PreTrainedTokenizerFast(name_or_path=''roberta-base'', vocab_size=50265,
    model_max_len=512, is_fast=True, padding_side=''right'', special_tokens={''bos_token'':
    ''<s>'', ''eos_token'': ''</s>'', ''unk_token'': ''<unk>'', ''sep_token'': ''</s>'',
    ''pad_token'': ''<pad>'', ''cls_token'': ''<s>'', ''mask_token'': AddedToken("<mask>",
    rstrip=False, lstrip=True, single_word=False, normalized=False)})'
train_batch_size:
  desc: null
  value: 64
valid_batch_size:
  desc: null
  value: 64
weight_decay:
  desc: null
  value: 1.0e-06
